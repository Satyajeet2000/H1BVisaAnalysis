# -*- coding: utf-8 -*-
"""SL-VI Mini Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i3amz8qOua_SqRrLZo_NdSbZW0nVnb-U

# **SL VI Mini Project**

## H-1B Visa Analysis
![image](https://d2v9ipibika81v.cloudfront.net/uploads/sites/114/2016/03/visa-759.jpg)

---
---

###### 33341 Krishiv Mewani
###### 33344 Harshad Manglani
###### 33351 Tanay Patankar
###### 33354 Satyajeet Patil

# Loading Datasets and installing and importing necessary libraries
"""

from google.colab import drive
drive.mount('/content/gdrive')

!mkdir datasets
!unzip gdrive/MyDrive/Datasets/h1b_dataset.zip -d datasets

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# import sys
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import plotly.graph_objects as go
# import plotly.offline as py
# from sklearn.preprocessing import LabelEncoder
# from sklearn.utils import resample
# import gc
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, accuracy_score
# from sklearn.ensemble import RandomForestClassifier
# from sklearn import metrics
# from sklearn.metrics import roc_auc_score
# 
# 
# !pip install plotly>=4.0.0
# !wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca
# !chmod +x /usr/local/bin/orca
# !apt-get install xvfb libgtk2.0-0 libgconf-2-4

cols = ['CASE_STATUS','EMPLOYMENT_START_DATE','EMPLOYER_NAME',
        'EMPLOYER_STATE','JOB_TITLE','SOC_NAME','PREVAILING_WAGE','PW_UNIT_OF_PAY']

# Importing the Datasets
df18 = pd.read_csv('datasets/h1b18.csv', usecols = cols)
df17 = pd.read_csv('datasets/h1b17.csv', usecols = cols)
df16 = pd.read_csv('datasets/h1b16.csv')

df16.head()

df17.head()

df18.head()

"""# Data Transformation

#### Selecting the necessary columns
"""

# Removing columns that are not necessary from data of df18 and df17
df17 = df17[['CASE_STATUS','EMPLOYMENT_START_DATE','EMPLOYER_NAME',
        'EMPLOYER_STATE','JOB_TITLE','SOC_NAME','PREVAILING_WAGE','PW_UNIT_OF_PAY']]

df18 = df18[['CASE_STATUS','EMPLOYMENT_START_DATE','EMPLOYER_NAME',
        'EMPLOYER_STATE','JOB_TITLE','SOC_NAME','PREVAILING_WAGE','PW_UNIT_OF_PAY']]

"""#### Standardising the format of the date"""

# Changing the EMPLOYMENT_START_DATE into a year to comply with 2016 data
df18['EMPLOYMENT_START_DATE'] = pd.to_datetime(df18['EMPLOYMENT_START_DATE'], format = '%m/%d/%y')
df18['EMPLOYMENT_START_DATE'] = df18['EMPLOYMENT_START_DATE'].apply(lambda x: x.year)

df17['EMPLOYMENT_START_DATE'] = pd.to_datetime(df17['EMPLOYMENT_START_DATE'], format = '%Y-%m-%d')
df17['EMPLOYMENT_START_DATE'] = df17['EMPLOYMENT_START_DATE'].apply(lambda x: x.year)

# Only take years 2017 from df17 and years 2018 from df18
df17 = df17.loc[df17['EMPLOYMENT_START_DATE'] == 2017, :]
df18 = df18.loc[df18['EMPLOYMENT_START_DATE'] == 2018, :]

"""#### Checking Missing Entries"""

print('Number of entries:', df16.shape[0])
print('Number of missing data in each column:')
print(df16.isnull().sum())

print('Number of entries:', df17.shape[0])
print('Number of missing data in each column:')
print(df17.isnull().sum())

print('Number of entries:', df18.shape[0])
print('Number of missing data in each column:')
print(df18.isnull().sum())

# Dropping the rows with nan values
df16 = df16.drop(['lon','lat'], axis=1)
df16 = df16.dropna()
df17 = df17.dropna()
df18 = df18.dropna()

"""Dataset is quite organized with nearly no missing relevent values in 2 of the files

#### Standardising the different formats of wages
"""

df17.PW_UNIT_OF_PAY.unique()

# Change PREVAILING WAGE into only yearly wages (Split into Year, Week, Month, Hourly, Bi-weekly)
df18['PREVAILING_WAGE'] = df18['PREVAILING_WAGE'].apply(lambda x: str(x).replace(',',''))
df18.loc[df18.PREVAILING_WAGE == 0, 'PREVAILING_WAGE'] = np.nan
df18.loc[df18.PW_UNIT_OF_PAY == 'Hour', 'PREVAILING_WAGE'] = df18.loc[df18.PW_UNIT_OF_PAY == 'Hour', 'PREVAILING_WAGE'].apply(lambda x:float(x)*1638)
df18.loc[df18.PW_UNIT_OF_PAY == 'Week', 'PREVAILING_WAGE'] = df18.loc[df18.PW_UNIT_OF_PAY == 'Week', 'PREVAILING_WAGE'].apply(lambda x:float(x)*52)
df18.loc[df18.PW_UNIT_OF_PAY == 'Bi-Weekly', 'PREVAILING_WAGE'] = df18.loc[df18.PW_UNIT_OF_PAY == 'Bi-Weekly', 'PREVAILING_WAGE'].apply(lambda x:float(x)*26)
df18.loc[df18.PW_UNIT_OF_PAY == 'Month', 'PREVAILING_WAGE'] = df18.loc[df18.PW_UNIT_OF_PAY == 'Month', 'PREVAILING_WAGE'].apply(lambda x:float(x)*12)

df17['PREVAILING_WAGE'] = df17['PREVAILING_WAGE'].apply(lambda x: str(x).replace(',',''))
df17.loc[df17.PREVAILING_WAGE == 0, 'PREVAILING_WAGE'] = np.nan
df17.loc[df17.PW_UNIT_OF_PAY == 'Hour', 'PREVAILING_WAGE'] = df17.loc[df17.PW_UNIT_OF_PAY == 'Hour', 'PREVAILING_WAGE'].apply(lambda x:float(x)*1638)
df17.loc[df17.PW_UNIT_OF_PAY == 'Week', 'PREVAILING_WAGE'] = df17.loc[df17.PW_UNIT_OF_PAY == 'Week', 'PREVAILING_WAGE'].apply(lambda x:float(x)*52)
df17.loc[df17.PW_UNIT_OF_PAY == 'Bi-Weekly', 'PREVAILING_WAGE'] = df17.loc[df17.PW_UNIT_OF_PAY == 'Bi-Weekly', 'PREVAILING_WAGE'].apply(lambda x:float(x)*26)
df17.loc[df17.PW_UNIT_OF_PAY == 'Month', 'PREVAILING_WAGE'] = df17.loc[df17.PW_UNIT_OF_PAY == 'Month', 'PREVAILING_WAGE'].apply(lambda x:float(x)*12)

# Remove PW_UNIT_OF_PAY
df17 = df17[['CASE_STATUS','EMPLOYMENT_START_DATE','EMPLOYER_NAME',
        'EMPLOYER_STATE','JOB_TITLE','SOC_NAME','PREVAILING_WAGE']]

df18 = df18[['CASE_STATUS','EMPLOYMENT_START_DATE','EMPLOYER_NAME',
        'EMPLOYER_STATE','JOB_TITLE','SOC_NAME','PREVAILING_WAGE']]

"""#### Converting locations into state codes"""

# Dictionary of all States and Shortened State
states = {"AL":"Alabama","AK":"Alaska","AZ":"Arizona","AR":"Arkansas","CA":"California","CO":"Colorado",
          "CT":"Connecticut","DC":"District Of Columbia","DE":"Delaware","FL":"Florida","GA":"Georgia","HI":"Hawaii","ID":"Idaho",
          "IL":"Illinois","IN":"Indiana","IA":"Iowa","KS":"Kansas","KY":"Kentucky","LA":"Louisiana",
          "ME":"Maine","MD":"Maryland","MA":"Massachusetts","MI":"Michigan","MN":"Minnesota","MS":"Mississippi",
          "MO":"Missouri","MT":"Montana","NE":"Nebraska","NV":"Nevada","NH":"New Hampshire","NJ":"New Jersey",
          "NM":"New Mexico","NY":"New York","NC":"North Carolina","ND":"North Dakota","OH":"Ohio","OK":"Oklahoma",
          "OR":"Oregon","PA":"Pennsylvania","PR":"Puerto Rico","RI":"Rhode Island","SC":"South Carolina","SD":"South Dakota",
          "TN":"Tennessee","TX":"Texas","UT":"Utah","VT":"Vermont","VA":"Virginia","WA":"Washington",
          "WV":"West Virginia","WI":"Wisconsin","WY":"Wyoming"}
states = dict((v.upper(), k.upper()) for k, v in states.items())

# Remove the city from the WORKSITE
df16['WORKSITE'] = df16['WORKSITE'].apply(lambda x: x.split(',')[1].strip())
# Replace Worksite with shortened States
df16['WORKSITE'].replace(states, inplace = True)
# Replace WORKSITE with EMPLOYER_STATE
df16.rename(columns={'YEAR':'EMPLOYMENT_START_DATE','WORKSITE':'EMPLOYER_STATE'}, inplace=True)


# Removing unnecessary columns of df16
df16 = df16[['CASE_STATUS','EMPLOYMENT_START_DATE','EMPLOYER_NAME',
        'EMPLOYER_STATE','JOB_TITLE','SOC_NAME','PREVAILING_WAGE']]

"""#### Integrating the different datasets"""

df = pd.concat([df16,df17,df18])
df = df.reset_index(drop = True)
df.head()

"""#### Final error correction"""

# Cleaning the SOC_NAME
df['SOC_NAME'] = df['SOC_NAME'].apply(lambda x: str(x).upper())
df['SOC_NAME'] = df['SOC_NAME'].apply(lambda x:str(x).replace('COMPUTER SYSTEMS ANALYSTS','COMPUTER SYSTEMS ANALYST'))

# Changing Wage to float
df['PREVAILING_WAGE'] = df['PREVAILING_WAGE'].astype(str).astype(float)

"""# Visualisations

#### Applicants Per Year
"""

# Looking at the number of applications per year
year_applicants = df['EMPLOYMENT_START_DATE'].value_counts()
print(year_applicants)

# Visualising the number of applications per year

year_applicants = year_applicants.sort_index()

obama = pd.Series(year_applicants.values[0:6], index = year_applicants.index[0:6])
trump = pd.Series(year_applicants.values[6:], index = year_applicants.index[6:])

fig, ax = plt.subplots()
plt.bar(obama.index, obama.values, align = 'center', alpha = 0.7,  linewidth=0)
plt.bar(trump.index, trump.values, align = 'center', alpha = 0.7,  linewidth=0, color = '#FF0000')
plt.legend(['Obama\'s Presidency','Trump\'s Presidency'],loc=2)
plt.title('The Number of Applications from 2011 to 2018')

plt.show()

"""From the series above, we can see that there are about 500,000 applicants every year. We can also see that there has been a decrease in the number of applications in 2017 and 2018 (Trump's Presidency) The figure below highlights the fall in the number of applicants in Trump's Presidency

#### Job Location
"""

# Organizing by state
df_heatmap = df.groupby('EMPLOYER_STATE')['CASE_STATUS'].count()

scl = [
    [0.0, 'rgb(242,240,247)'],
    [0.2, 'rgb(218,218,235)'],
    [0.4, 'rgb(188,189,220)'],
    [0.6, 'rgb(158,154,200)'],
    [0.8, 'rgb(117,107,177)'],
    [1.0, 'rgb(84,39,143)']
]

fig = go.Figure(
    data=go.Choropleth(
      locations=df_heatmap.index, # Spatial coordinates
      z = df_heatmap.values.astype(float), # Data to be color-coded
      locationmode = 'USA-states', # set of locations match entries in `locations`
      marker = go.choropleth.Marker(
          line = go.choropleth.marker.Line(
              color = 'rgb(255,255,255)',
              width = 2
          )),
      colorscale = scl,
      autocolorscale = False,
      colorbar = go.choropleth.ColorBar(
          title = "")
    ),
    layout = go.Layout(
      title = go.layout.Title(
          text = 'H1B Visa Applicants'
      ),
      geo = go.layout.Geo(
          scope = 'usa',
          projection = go.layout.geo.Projection(type = 'albers usa'),
          showlakes = True,
          lakecolor = 'rgb(255, 255, 255)'),
    )
  )

# fig.show(renderer="png")
fig.show(renderer="colab")

"""This choropleth map highlights the location of applicants. From the figure, we can see that the majority of the H-1B visa applicants are applying from California, New York, and Texas.

#### Acceptance Status
"""

# Comparing the number of applications that are accepted or denied (Ignore withdrawn)
accept_arr = df['CASE_STATUS'].value_counts()
print(accept_arr)
accept_val = list(accept_arr)[0:4]
accept_label = list(accept_arr.index)[0:4]

# Out of all the applications we can see that only a small percentage are denied (Exactly 2.67%)
denied = (accept_arr['DENIED'] / df.shape[0]).round(4)
print(str(denied*100) + '%')

tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),
             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),
             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),
             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),
             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]

for i in range(len(tableau20)):
    r, g, b = tableau20[i]
    tableau20[i] = (r / 255., g / 255., b / 255.)

plt.figure(figsize=(10,7))
df.CASE_STATUS.value_counts().plot(kind='barh',  color=tableau20)
df.sort_values('CASE_STATUS')
plt.title("NUMBER OF APPLICATIONS")
plt.show()

# Pie chart of accepted and denied applications
fig, ax = plt.subplots()
ax.pie(accept_val, explode = (0,0,0,0.25), labels = accept_label, autopct = '%1.1f%%',
       pctdistance=1.2, labeldistance=1.4, wedgeprops={'alpha':0.7})
ax.axis('equal')
plt.title('Case Status')
plt.show()

"""#### Applicant Company Analysis"""

fig, ax = plt.subplots()
ax = df['EMPLOYER_NAME'].groupby([df['EMPLOYER_NAME']]).count().sort_values(ascending=False).head(10).plot(kind='barh', title="Top 10 Applicant over 2011 to 2018")
ax.set_ylabel("")
plt.show()

topEmp = list(df['EMPLOYER_NAME'][df['EMPLOYMENT_START_DATE'] >= 2015].groupby(df['EMPLOYER_NAME']).count().sort_values(ascending=False).head(10).index)
byEmpYear = df[['PREVAILING_WAGE', 'EMPLOYER_NAME', 'EMPLOYMENT_START_DATE']][df['EMPLOYER_NAME'].isin(topEmp)]
byEmpYear = byEmpYear.groupby([df['EMPLOYER_NAME'],df['EMPLOYMENT_START_DATE']])

markers=['o','v','^','<','>','d','s','p','*','h','x','D','o','v','^','<','>','d','s','p','*','h','x','D']
fig = plt.figure(figsize=(12,7))
for company in topEmp:
    tmp = byEmpYear.count().loc[company]
    plt.plot(tmp.index.values, tmp["PREVAILING_WAGE"].values, label=company, linewidth=2,marker=markers[topEmp.index(company)])
plt.xlabel("Year")
plt.ylabel("Number of Applications")
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Number of Applications of Top 10 Applicants')
plt.show()

"""- INFOSIS shows a very rapid development, especially during period from 2011 to 2013, where it came from about zero to 32k applications and a rapid decline 2016 onwards.
- TATA also shows a significant development.
- Most of the top applicants are from India.

#### Jobs Analysis
"""

PopJobs = df[['JOB_TITLE', 'EMPLOYER_NAME', 'PREVAILING_WAGE']][df['EMPLOYER_NAME'].isin(topEmp)].groupby(['JOB_TITLE'])
topJobs = list(PopJobs.count().sort_values(by='EMPLOYER_NAME', ascending=False).head(30).index)
df_2 = PopJobs.count().loc[topJobs].assign(mean_wage=PopJobs.mean().loc[topJobs])
fig = plt.figure(figsize=(10,12))
ax1 = fig.add_subplot(111)
ax2 = ax1.twiny()
width = 0.35
df_2.EMPLOYER_NAME.plot(kind='barh', ax=ax1, color='C0', width=0.4, position=0, label='# of Applications')
df_2.mean_wage.plot(kind='barh', ax=ax2, color='C7', width=0.4, position=1, label='Mean Salary')
ax1.set_xlabel('Number of Applications')
ax1.set_ylabel('')
ax1.legend(loc=(0.75,0.75))
ax2.set_xlabel('Mean Salary')
ax2.set_ylabel('Job Title')
ax2.legend(loc=(0.75,0.70))
plt.show()

"""Top jobs from top 10 applications are mostly IT jobs. The most popular jobs are "Technology Lead" and "Technology Analyst" but their offering salary are not top, they are about middle. The positions offering the highest salary are "Analyst" and "Manager", which is absolutely understandable.

# Feature Engineering

### Finding and removing outliers
"""

df['PREVAILING_WAGE'].max()

df = df[df['PREVAILING_WAGE'] <= 500000]

"""#### Generalising the job categories"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import sys
# 
# df['SOC_NAME1'] = 'others'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('computer','software')] = 'it'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('software|web')] = 'it'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('chief','management')] = 'manager'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('manager')] = 'manager'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('mechanical')] = 'mechanical'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('database')] = 'database'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('sales','market')] = 'scm'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('financial')] = 'finance'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('treasure')] = 'finance'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('public','fundraising')] = 'pr'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('education','law')] = 'administrative'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('auditors','compliance')] = 'audit'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('distribution','logistics')] = 'scm'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('recruiters','human')] = 'hr'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('agricultural','farm')] = 'agri'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('construction','architectural')] = 'estate'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('forencsic','health')] = 'medical'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('bio')] = 'medical'
# df['SOC_NAME1'][df['SOC_NAME'].str.lower().str.contains('teachers')] = 'education'

# Garbage Collection
gc.collect()

df['SOC_NAME1'].value_counts()

"""#### Encoding the Labels"""

labelencoder_1 = LabelEncoder()

# df['CASE_STATUS_enc'] = labelencoder_1.fit_transform(df['CASE_STATUS'])
df['CASE_STATUS_enc'] = df['CASE_STATUS'].map({'CERTIFIED' : 1, 'CERTIFIED-WITHDRAWN' : 1, 'DENIED' : 0, 'WITHDRAWN' : 0,
                                           'PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED' : 0, 'REJECTED' : 0, 'INVALIDATED' : 0})

labelencoder_2 = LabelEncoder()
df['EMPLOYER_STATE_enc'] = labelencoder_2.fit_transform(df['EMPLOYER_STATE'].astype(str))

labelencoder_3 = LabelEncoder()
df['SOC_NAME_enc'] = labelencoder_3.fit_transform(df['SOC_NAME1'].astype(str))
df

"""#### Removing unnecessary columns"""

df.columns

df_final = df.drop(['CASE_STATUS','EMPLOYER_NAME', 'SOC_NAME','SOC_NAME1','JOB_TITLE','EMPLOYER_STATE'], axis = 1)
df_final

"""#### Calculating the Coorelation"""

sns.heatmap(df_final.corr(), annot=True, cmap="RdYlGn", annot_kws={"size":15})

"""Variables are weakly coorelated. So we will not remove any of the variables

### Dividing into dependant and independant variables
"""

x = df_final.drop(['CASE_STATUS_enc'], axis=1) # Independent variables
y = df_final['CASE_STATUS_enc'] # Dependent variables

"""# Model Building

## 1. Logistic Regression with Upscaled dataset

The dataset is highly imbalanced. To ensure that the minority class gets trained, we can upscale it to increase its accuracy. The downside of this is overfitting.
"""

# Separate majority and minority classes
df_majority = df_final[df_final.CASE_STATUS_enc==1]
df_minority = df_final[df_final.CASE_STATUS_enc==0]

# Upsample minority class
df_minority_upsampled = resample(df_minority,
                                 replace=True,     # sample with replacement
                                 n_samples=3856979,    # to match majority class
                                 random_state=123) # reproducible results

# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_majority, df_minority_upsampled])

# Display new class counts
df_upsampled.CASE_STATUS_enc.value_counts()

x = df_upsampled.drop(['CASE_STATUS_enc'], axis=1) # Independent variables
y = df_upsampled['CASE_STATUS_enc'] # Dependent variables
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1234)

lr_model = LogisticRegression()
lr_model_results = lr_model.fit(x_train, y_train)
y_pred = lr_model.predict(x_test)

# Is our model still predicting just one class?
print( np.unique( y_pred ) )

# How's our accuracy?
print( accuracy_score(y_test, y_pred) )

# AUC ROC Score
print(roc_auc_score(y_test, y_pred) )

print(classification_report(y_test, y_pred))

"""With an upscaled dataset, the accuracy is quite low, at only 53%

## 2. Random Forest Classifier
Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes.

In modern applied machine learning, tree ensembles (Random Forests, etc.) almost always outperform singular decision trees, so we'll use them directly.
"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 32)

rf_model = RandomForestClassifier()
rf_model_results = rf_model.fit(x_train, y_train)
y_pred = rf_model.predict(x_test)

# Is our model still predicting just one class?
print( np.unique( y_pred ) )

# How's our accuracy?
print( accuracy_score(y_test, y_pred) )

# AUC ROC Score
print(roc_auc_score(y_test, y_pred) )

print(classification_report(y_test, y_pred))

"""Random forest chooses features randomly during the training process. Therefore, it does not depend highly on any specific set of features. Therefore, the random forest can generalize over the data in a better way

#### Ignore: Saving Models for reduced resource usage
"""

!mkdir gdrive/MyDrive/Datasets/models

import pickle
model_name = 'rfc_model'
filename = 'gdrive/MyDrive/Datasets/models/' + model_name + '.sav'
# pickle.dump(model_name, open(filename, 'wb'))

# # load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.predict(Y_test)
print(result)

loaded_model

